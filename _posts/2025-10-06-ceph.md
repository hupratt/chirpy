---
title: k8s part 5 persistent storage
author: hugo
date: 2025-10-09 09:11:00 +0200
categories: [Tutorial, infrastructure]
tags: [sysadmin, networking, k8s, ceph, storage]
render_with_liquid: false
---

### Introduction

I'd been testing ways to persist data in kubernetes and considered the mariadb's galera cluster, zfs, long horn, btrfs and all of these would most likely work for my purposes but they were all lacking in one point or another. The galera cluster didn't seem optimized for write intensive applications and required a lot of helpers and (external?) proxies to keep in sync. Synchronizing data accross nodes with snapshots did not seem like a solution that I could easily document and trust it to just anyone so btrfs and zfs were out due to complexity. Long horn seemed like a solid option as well but I did not want to choose a solution that would limit ourselves to only block storage. 

Ceph on the other hand seemed to tick all the boxes, it works out of the box, it's future proof, decoupled from the kubernetes eco-system and life cycle, and I wouldn't have to make sure the changes are synced before moving pods around. Looking back I think it was the right choice and I wouldn't say the setup was hard, quite the opposite.

In this article I'll build on top of the [3 node k8s on hetzner's bare metal machines](https://chirpy.thekor.eu/posts/k8s-p3/) and add ceph to the 6 available partitions we setup on the cloud machines for that purpose. 


### Setting up ceph

#### Overview

Ceph, or should I say "rook Ceph"? Has a series of compulsory and optional services. These are the three compulsory ones:

1. ceph-mon: stands for monitor: It keeps the cluster map and state (whoâ€™s in the cluster and which OSDs exist) + ensures consensus via quorum. This service communicates on TCP 6789
2. ceph-OSD: stands for Object Storage Daemon: It handles actual data storage. Each OSD maps to a disk and stores objects, replicates data, recovers peers, and reports to the ceph-mon's. Between OSDs this service communicates over TCP ports 6800-7300.
3. ceph-MGR/HTTP: stands for Manager: Collects runtime metrics, provides a dashboard, a prometheus endpoint, and runs management modules. It is available at TCP 9283

#### The firewall

The following ports should be open on the vlan private network:

```bash
firewall-cmd --zone=myprivate --add-port=6789/tcp --permanent 
firewall-cmd --zone=myprivate --add-port=6800-7300/tcp --permanent
firewall-cmd --zone=myprivate --add-port=9283/tcp --permanent 
firewall-cmd --reload
```

#### The installation

```bash
git clone --single-branch --branch v1.15.3 https://github.com/rook/rook.git
cd rook/deploy/examples
kubectl create namespace rook-ceph
kubectl apply -f crds.yaml -f common.yaml -f operator.yaml
```

copy over the existing template and edit it
```bash
cp cluster.yaml ceph-cluster.yml
nano ceph-cluster.yml
(...)
  storage: # cluster level storage configuration and selection
    useAllNodes: false
    useAllDevices: false
    nodes:
    - name: "kube-node1"
      devices:
        - name: "/dev/nvme0n1p3"
        - name: "/dev/nvme1n1p3"
    - name: "kube-node2"
      devices:
        - name: "/dev/nvme0n1p3"
        - name: "/dev/nvme1n1p3"
    - name: "kube-node3"
      devices:
        - name: "/dev/nvme0n1p3"
        - name: "/dev/nvme1n1p3"
(...)
```
Make sure the partition has no formatting by running 

```bash
blkid /dev/nvme1n1p3
fdisk /dev/nvme1n1
```

If it does you can remove it

```bash
umount /dev/nvme1n1p3 || true
wipefs -a /dev/nvme1n1p3
```

And start the formatting and ceph bootstrap:

```bash
kubectl apply -f ceph-cluster.yml
kubectl apply -f csi/rbd/storageclass.yaml
```

Deploy the toolbox to be able to query the ceph state

```bash
kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/deploy/examples/toolbox.yaml -n rook-ceph
kubectl get pods -n rook-ceph -l app=rook-ceph-tools

kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- bash
ceph status
ceph health detail
ceph osd tree
ceph df
```

If the setup fails you can restart the job by running these lines:

```bash
kubectl -n rook-ceph delete job -l app=rook-ceph-osd-prepare
kubectl -n rook-ceph delete pod -l app=rook-ceph-operator
kubectl -n rook-ceph logs -l app=rook-ceph-osd-prepare
```

Once the CephFS is showing up as healthy you can create the PVC and apply it to the database:

```bash
cat <<EOF > test-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-pvc
  namespace: cert-manager
spec:
  storageClassName: rook-ceph-block
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
EOF
kubectl apply -f test-pvc.yaml
kubectl get pvc
```

And apply the persistent volume [to the mariadb deployment](https://raw.githubusercontent.com/hupratt/kubernetes-the-hard-way/refs/heads/v1/kubeconfiguration/1-mongo-database.yml) like we did [on a previous article](https://chirpy.thekor.eu/posts/k8s-p2/#setting-up-a-mongo-db-and-a-web-app).


```yaml
(...)
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongo
  template:
    metadata:
      labels:
        app: mongo
    spec:
      containers:
      - name: mongo
        (...)
        volumeMounts:
        - name: mongodb-storage
          mountPath: /data/db
      volumes:
      - name: mongodb-storage
        persistentVolumeClaim:
          claimName: test-pvc
```

You can verify it worked by deleting all database pods or by deleting the deployment and recreating it

```bash
kubectl delete pods -l app=mongo -n cert-manager
kubectl delete deployment/mongodb-deployment
kubectl apply -f https://git.adata.de/adata/technik/ceph/-/raw/adata/deploy/examples/mongo.yaml?ref_type=heads
```