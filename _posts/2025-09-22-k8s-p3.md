---
title: 3 node k8s on hetzner's bare metal machines
author: hugo
date: 2025-09-22 09:11:00 +0200
categories: [Tutorial, infrastructure]
tags: [sysadmin, networking, k8s]
render_with_liquid: false
---

### Introduction

This is the third one in the series. We looked at how to install, configure kubernetes and deploy a web app with it. Now we'll take setup to another level with 3 control planes and 3 workers on a 3 node cluster. 

### Install the OS

#### Regular rocky linux 10 with ext4 partition

If you're in a rush you can install the OS through hetzner's tool. Documentation on hetzner's tool [can be found here](https://docs.hetzner.com/robot/dedicated-server/operating-systems/installimage/).

To run a fresh install run the command below, choose the distro and press F10 to apply

```bash
root@rescue:$ installimage -l config
```

You can also give it a configuration file and you don't need to type anything else manually

```bash

root@rescue:$ cat <<EOF > install-config.txt
DRIVE1 /dev/nvme0n1

DRIVE2 /dev/nvme1n1

SWRAID 1

SWRAIDLEVEL 1 # Use 1 for Raid 1

HOSTNAME k8s-node01.example.com # Set correct hostname

IPV4_ONLY no

USE_KERNEL_MODE_SETTING yes

PART /boot ext3 512M
PART / ext4 32G

IMAGE /root/.oldroot/nfs/install/../images/Rocky-1000-amd64-base.tar.gz
EOF
```


```bash
root@rescue:$ installimage -a -c install-config.txt
```

If everything went well you should see a summary and simply type reboot

#### Fedora server 42 with btrfs on root

Btrfs has the advantage of being a lot faster than rsync so if you have a lot of data and you're planning on doing remote backups over a flacky VPN connection this is the setup you want instead of the ext4 filesystem. 

I'll create an ext4 partition for the grub boot files and a btrfs raid 1 with the available space that is left so that we can btrfs send snapshots to a remote backup location. I wrote an article on using the btrfs send command already. [Check it out here](https://chirpy.thekor.eu/posts/live-backups-with-btrfs/) if you're interested

Prepare the rootfs from a fedora 42 workstation you might have laying around. 

```bash
mkdir -p /home/hugo/fedora-rootfs

sudo dnf --releasever=42 --installroot=/home/hugo/fedora-rootfs \
    --setopt=install_weak_deps=False \
    --use-host-config \
    install -y bash passwd dnf fedora-release systemd sudo cloud-init

sudo rm -rf ~/fedora-rootfs/var/cache/dnf
cd ~/fedora-rootfs
sudo tar -cJf ~/fedora-rootfs.tar.xz .
```

Upload the rootfs to an s3 bucket and boot the hetzner machine into rescue mode

![iframe](</assets/img/posts/swappy-20251002-101921.png>)


```bash
# Partition disk
parted /dev/nvme0n1 --script mklabel gpt
parted /dev/nvme0n1 --script mkpart primary 1MiB 2MiB
parted /dev/nvme0n1 --script set 1 bios_grub on
parted /dev/nvme0n1 --script mkpart primary 2MiB 522MiB
parted /dev/nvme0n1 --script mkpart primary 522MiB 100%

parted /dev/nvme1n1 --script mklabel gpt
parted /dev/nvme1n1 --script mkpart primary 1MiB 2MiB
parted /dev/nvme1n1 --script set 1 bios_grub on
parted /dev/nvme1n1 --script mkpart primary 2MiB 522MiB
parted /dev/nvme1n1 --script mkpart primary 522MiB 100%
```

If the command above failed it probably means that you're running some kind of raid already in the background

```bash
# deactivate LVM volume groups
vgchange -an
mdadm --stop /dev/md0
mdadm --stop /dev/md1
mdadm --stop /dev/md....
```

If you didn't get an error proceed with the installation:

```bash
mdadm --create --verbose /dev/md0 --level=1 --raid-devices=2 /dev/nvme0n1p2 /dev/nvme1n1p2
mkfs.ext4 /dev/md0

# Format and setup raid
mkfs.btrfs -f -d raid1 -m raid1 /dev/nvme0n1p3 /dev/nvme1n1p3
mount /dev/nvme0n1p3 /mnt

btrfs filesystem df /mnt
Data, RAID1: total=1.00GiB, used=0.00B
System, RAID1: total=8.00MiB, used=16.00KiB
Metadata, RAID1: total=1.00GiB, used=128.00KiB
GlobalReserve, single: total=5.50MiB, used=0.00B

btrfs subvolume create /mnt/root
umount /mnt
mount -o subvol=root /dev/nvme0n1p3 /mnt/
btrfs filesystem df /mnt

# apply the rootfs
wget https://minio-api.thekor.eu/public-f1492f08-f236-4a55-afb7-70ded209cb27/fedora-rootfs.tar.xz
tar -xJf fedora-rootfs.tar.xz -C /mnt

# set up the fstab
BOOT_UUID=$(blkid -s UUID -o value /dev/md0)
ROOT_UUID=$(blkid -s UUID -o value /dev/nvme0n1p3)

cat <<EOF > /mnt/etc/fstab
UUID=$ROOT_UUID / btrfs subvol=root,compress=zstd:1,x-systemd.device-timeout=0 0 0
UUID=$BOOT_UUID /boot ext4 defaults 0 0
EOF

# download the pub key
mkdir -p /mnt/root/.ssh
chmod 700 /mnt/root/.ssh
curl -fsSL https://minio-api.thekor.eu/public-f1492f08-f236-4a55-afb7-70ded209cb27/id_ed25519.pub > /mnt/root/.ssh/authorized_keys
chmod 600 /mnt/root/.ssh/authorized_keys

# ssh config
cat <<EOF > /mnt/etc/ssh/sshd_config
Include /etc/ssh/sshd_config.d/*.conf
AuthorizedKeysFile	.ssh/authorized_keys
PermitRootLogin prohibit-password
EOF

# temporary DNS to be able to set up the image properly
cat > /mnt/etc/resolv.conf << 'EOF'
nameserver 1.1.1.1
nameserver 8.8.8.8
nameserver 8.8.4.4
options rotate
options timeout:2
EOF

# Enable open ssh server
# mount -o subvol=root /dev/nvme0n1p3 /mnt/
mount --bind /dev /mnt/dev
mount --bind /proc /mnt/proc
mount --bind /sys /mnt/sys
mount --bind /dev/pts /mnt/dev/pts
mount /dev/md0 /mnt/boot
chroot /mnt /bin/bash -c "dnf install -y openssh-server && systemctl enable sshd && dnf install -y grub2 grub2-tools"

# Install GRUB BIOS
chroot /mnt /bin/bash -c "dnf install -y mdadm btrfs-progs && mdadm --detail --scan"
chroot /mnt /bin/bash -c "grub2-install --target=i386-pc /dev/nvme0n1"
chroot /mnt /bin/bash -c "grub2-install --target=i386-pc /dev/nvme1n1"
chroot /mnt /bin/bash -c "grub2-mkconfig -o /boot/grub2/grub.cfg"

# generate the /boot/initramfs
chroot /mnt /bin/bash -c "dnf install -y kernel kernel-core kernel-modules && dracut --regenerate-all --force"
# set a password otherwise you won't be able to login as the root is locked without a password
chroot /mnt /bin/bash -c "passwd root"

# Install and configure networking in chroot
chroot /mnt /bin/bash <<'EOF'
set -e

# Install NetworkManager
dnf install -y NetworkManager nano

# Enable it at boot
systemctl enable NetworkManager

# Add a DHCP connection for ens18 (IPv4 only)
nmcli con add type ethernet ifname ens18 con-name ens18 ipv4.method auto ipv6.method ignore

# Bring it up immediately
nmcli con up ens18
EOF

# Clean up
umount -R /mnt
reboot
```

If you shutdown your machine instead, make sure not to trigger the hardware reset while off because you'll lose access to the machine and have to wait for a technician to manually start it


### Configure the nodes

setup fail2ban since you're opening ssh to the world wide web

```bash
root@node0X:$ hostnamectl set-hostname k8s-node0X
root@node0X:$ dnf install systemd-resolved -y
root@node0X:$ systemctl enable --now systemd-resolved
root@node0X:$ ln -sf /run/NetworkManager/resolv.conf /run/systemd/resolve/resolv.conf
root@node0X:$ cat <<EOF >> /etc/hosts
10.210.19.11 kube-node1
10.210.19.12 kube-node2
10.210.19.13 kube-node3
EOF
```

Setup time synchronization

```bash
root@node0X:$ timedatectl set-timezone Europe/Berlin
root@node0X:$ timedatectl
root@node0X:$ dnf install -y chrony
root@node0X:$ systemctl enable --now chronyd
root@node0X:$ cat <<EOF >> /etc/chrony.conf
# German NTP servers
server 0.de.pool.ntp.org iburst
server 1.de.pool.ntp.org iburst
server 2.de.pool.ntp.org iburst
server 3.de.pool.ntp.org iburst
EOF
root@node0X:$ systemctl restart chronyd
root@node0X:$ chronyc tracking
root@node0X:$ firewall-cmd --zone=public --add-service=ntp --permanent && firewall-cmd --reload
```

Setup jails for ssh

```bash
root@node0X:$ dnf upgrade -y && dnf install fail2ban -y 
root@node0X:$ dnf install epel-release -y 

# required for the ansible script to run
root@node0X:$ dnf install -y python3-libdnf5
root@node0X:$ systemctl status fail2ban.service
root@node0X:$ cp /etc/fail2ban/jail.conf /etc/fail2ban/jail.local
root@node0X:$ nano /etc/fail2ban/jail.local
[sshd]
(...)
enabled = true
maxretry = 5
```
Start and enable the service

```bash
root@node0X:$ systemctl enable --now fail2ban
root@node0X:$ fail2ban-client status sshd
```

Set up the network overlay so that the nodes can reach each other

```bash
root@node01:$ nmcli connection add type vlan con-name vlan4000 dev enp9s0 id 4000
root@node01:$ nmcli connection modify vlan4000 802-3-ethernet.mtu 1400
root@node01:$ nmcli connection modify vlan4000 ipv4.method manual ipv4.addresses "10.210.19.11/20"
root@node01:$ nmcli connection up vlan4000
root@node01:$ ip link set enp9s0.4000 name vlan4000
# set the vlan id
root@node01:$ nano /etc/NetworkManager/system-connections/vlan4000.nmconnection 
[connection]
id=vlan4000
(...)
interface-name=vlan4000
```


On node 2:

```bash
root@node02:$ nmcli connection add type vlan con-name vlan4000 dev enp41s0 id 4000
root@node02:$ nmcli connection modify vlan4000 802-3-ethernet.mtu 1400
root@node02:$ nmcli connection modify vlan4000 ipv4.method manual ipv4.addresses "10.210.19.12/20"
root@node02:$ nmcli connection up vlan4000
root@node02:$ ip link set enp41s0.4000 name vlan4000
# set the vlan id
root@node02:$ nano /etc/NetworkManager/system-connections/vlan4000.nmconnection 
[connection]
id=vlan4000
(...)
interface-name=vlan4000
```

On node 3:

```bash
root@node03:$ nmcli connection add type vlan con-name vlan4000 dev enp35s0 id 4000
root@node03:$ nmcli connection modify vlan4000 802-3-ethernet.mtu 1400
root@node03:$ nmcli connection modify vlan4000 ipv4.method manual ipv4.addresses "10.210.19.13/20"
root@node03:$ nmcli connection up vlan4000
root@node03:$ ip link set enp35s0.4000 name vlan4000
# set the vlan id
root@node03:$ nano /etc/NetworkManager/system-connections/vlan4000.nmconnection 
[connection]
id=vlan4000
(...)
interface-name=vlan4000
```

On node 1:

```bash
root@node01:$ dnf install python3 python3-pip git vi vim -y
root@node01:$ git clone https://github.com/kubernetes-sigs/kubespray.git
root@node01:$ git checkout tags/v2.28.1
root@node01:$ cd kubespray/
root@node01:$ python3 -m venv env
root@node01:$ source env/bin/activate
root@node01:$ pip install -r requirements.txt
root@node01:$ python -c "import sys; print(sys.version)" > release.txt
root@node01:$ cp -rfp inventory/sample inventory/mycluster
# remove the inventory from the gitignore so that we can keep track of what changed
root@node01:$ nano .gitignore
root@node01:$ echo "" > inventory/mycluster/inventory.ini
root@node01:$ nano inventory/mycluster/inventory.ini
[kube_control_plane]
kube-node1 ansible_host=10.210.19.11 ip=10.210.19.11 etcd_member_name=etcd1
kube-node2 ansible_host=10.210.19.12 ip=10.210.19.12 etcd_member_name=etcd2
kube-node3 ansible_host=10.210.19.13 ip=10.210.19.13 etcd_member_name=etcd3

[etcd:children]
kube_control_plane

[kube_node]
kube-node1 ansible_host=10.210.19.11
kube-node2 ansible_host=10.210.19.12
kube-node3 ansible_host=10.210.19.13

[k8s_cluster:children]
kube_control_plane
kube_node
```

Configure the cluster: 

```bash
(...)
root@node01:$ nano inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml
# Use Flannel as CNI
kube_network_plugin: flannel

# Flannel backend type
flannel_backend_type: vxlan

# MTU for Flannel to match Hetzner VLAN
flannel_mtu: 1400

kube_version: v1.33.5
helm_enabled: true
kube_proxy_mode: iptables

# SVC network 10.233.0.1 -> 10.233.63.254
kube_service_addresses: 10.233.0.0/18
# Pod network and service CIDRs 10.233.64.1 -<> 10.233.127.254
kube_pods_subnet: 10.233.64.0/18
```

Configure the flannel CNI: 

```bash
root@node01:$ nano inventory/mycluster/group_vars/k8s_cluster/k8s-net-flannel.yml
(...)
flannel_interface: "{{ hostvars[inventory_hostname]['k8s_node_vlan_iface'] }}"
```

test out if the variables are being read in properly

```bash
root@node01:$ ansible kube-node1 -i inventory/mycluster/inventory.ini -m debug -a "var=inventory_hostname"
root@node01:$ ansible kube-node1 -i inventory/mycluster/inventory.ini -m debug -a "var=k8s_node_vlan_iface"
root@node01:$ ansible kube-node1 -i inventory/mycluster/inventory.ini -m debug -a "var=hostvars[inventory_hostname]['k8s_node_vlan_iface']"
```



change the default nodelocaldns_ip and append the cluster dns here: 

```bash
root@node01:$ inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml
(...)
nodelocaldns_ip: 10.233.0.10
cluster_dns: 10.233.0.10
```

Specify the user ansible will use to SSH into the remote hosts.

```bash
root@node01:$ nano inventory/mycluster/group_vars/all/all.yml
(...)
ansible_ssh_user: root
ansible_ssh_private_key_file: /root/.ssh/id_ed25519
```


```bash
root@node01:$ ssh-keygen
```
and copy over the public and private key to the 3 nodes ~/.ssh/authorized_keys

This should say ok for the 3 nodes

```bash
root@node01:$ ansible -i inventory/mycluster/inventory.ini all -m ping
```

#### A. firewall setup

Create a private zone, assign it to the VLAN interface and allow all traffic within that zone


```bash
root@node01:$ firewall-cmd --permanent --new-zone=private
root@node01:$ nmcli connection modify vlan4000 connection.zone private
root@node01:$ nmcli connection down vlan4000 
root@node01:$ nmcli connection up vlan4000 
root@node01:$ firewall-cmd --permanent --zone=public --remove-interface=enp9s0.4000 
root@node01:$ firewall-cmd --permanent --zone=private --add-rich-rule='rule family="ipv4" source address="10.0.0.0/8" accept'
root@node01:$ firewall-cmd --permanent --zone=private --change-interface=enp9s0.4000
root@node01:$ firewall-cmd --reload
root@node01:$ systemctl restart firewalld && systemctl status firewalld
root@node01:$ firewall-cmd --set-default-zone=private
root@node01:$ firewall-cmd --permanent --zone=private --remove-interface=enp9s0
root@node01:$ firewall-cmd --permanent --zone=public --change-interface=enp9s0
root@node01:$ nmcli con modify 'Wired connection 1' ipv4.ignore-auto-dns yes
root@node01:$ nmcli con modify 'Wired connection 1' ipv4.dns "8.8.8.8 1.1.1.1"
root@node01:$ nmcli con up 'Wired connection 1'
```

```bash
root@node02:$ firewall-cmd --permanent --new-zone=private
root@node02:$ nmcli connection modify vlan4000 connection.zone private
root@node02:$ nmcli connection down vlan4000 
root@node02:$ nmcli connection up vlan4000 
root@node02:$ firewall-cmd --permanent --zone=public --remove-interface=enp41s0.4000 
root@node02:$ firewall-cmd --permanent --zone=private --add-rich-rule='rule family="ipv4" source address="10.0.0.0/8" accept'
root@node02:$ firewall-cmd --permanent --zone=private --change-interface=enp41s0.4000
root@node02:$ firewall-cmd --reload
root@node02:$ systemctl start firewalld && systemctl status firewalld
root@node02:$ firewall-cmd --set-default-zone=private
root@node02:$ firewall-cmd --permanent --zone=private --remove-interface=enp35s0
root@node02:$ firewall-cmd --permanent --zone=public --change-interface=enp35s0
root@node02:$ nmcli con modify 'cloud-init enp41s0' ipv4.ignore-auto-dns yes
root@node02:$ nmcli con modify 'cloud-init enp41s0' ipv4.dns "8.8.8.8 1.1.1.1"
root@node02:$ nmcli con up 'cloud-init enp41s0'
```

```bash
root@node03:$ firewall-cmd --permanent --new-zone=private
root@node03:$ nmcli connection modify vlan4000 connection.zone private
root@node03:$ nmcli connection down vlan4000 
root@node03:$ nmcli connection up vlan4000 
root@node03:$ firewall-cmd --permanent --zone=public --remove-interface=enp35s0.4000 
root@node03:$ firewall-cmd --permanent --zone=private --add-rich-rule='rule family="ipv4" source address="10.0.0.0/8" accept'
root@node03:$ firewall-cmd --permanent --zone=private --change-interface=enp35s0.4000
root@node03:$ firewall-cmd --reload
root@node03:$ systemctl restart firewalld && systemctl status firewalld
root@node03:$ firewall-cmd --set-default-zone=private
root@node03:$ firewall-cmd --permanent --zone=private --remove-interface=enp41s0
root@node03:$ firewall-cmd --permanent --zone=public --change-interface=enp41s0
root@node03:$ nmcli con modify 'cloud-init enp35s0' ipv4.ignore-auto-dns yes
root@node03:$ nmcli con modify 'cloud-init enp35s0' ipv4.dns "8.8.8.8 1.1.1.1"
root@node03:$ nmcli con up 'cloud-init enp35s0'
```

remove ssh and cockpit access from "anywhere". We also don't need dhcp service on the servers so we'll turn it off

```bash
root@node0X:$ firewall-cmd --zone=public --permanent --remove-service=cockpit
root@node0X:$ firewall-cmd --zone=public --permanent --remove-service=ssh
root@node0X:$ firewall-cmd --zone=public --permanent --remove-service=dhcpv6-client
```

```bash
root@node0X:$ firewall-cmd --zone=public --permanent \
  --add-rich-rule='rule family=ipv4 source address=10.0.0.0/8 service name=ssh accept' \
  --add-rich-rule='rule family=ipv4 source address=172.16.0.0/12 service name=ssh accept' \
  --add-rich-rule='rule family=ipv4 source address=192.168.0.0/16 service name=ssh accept' \
  --add-rich-rule='rule family=ipv4 source address=93.92.117.123 service name=ssh accept' \
  --add-rich-rule='rule family=ipv4 source address=185.224.117.254 service name=ssh accept'
root@node0X:$ firewall-cmd --zone=private --add-port=22/tcp --permanent
root@node0X:$ firewall-cmd --zone=private --permanent --add-rich-rule='rule family="ipv4" protocol value="icmp" accept'
root@node0X:$ firewall-cmd --zone=private --add-forward --permanent
root@node0X:$ firewall-cmd --reload
root@node0X:$ firewall-cmd --list-all
# verifu it got applied correctly
root@node0X:$ firewall-cmd --get-zones
root@node0X:$ firewall-cmd --get-active-zones
```

#### B. permit VLAN traffic

Allow NATing. You should set the target to DROP otherwise a default ACCEPT policy ignores all your firewall rules and simply allows everything to go through 

```bash
root@node0X:$ firewall-cmd --zone=public --add-port=53/udp --permanent
root@node0X:$ firewall-cmd --permanent --zone=public --set-target=DROP
root@node0X:$ firewall-cmd --permanent --zone=private --set-target=DROP
root@node0X:$ firewall-cmd --permanent --zone=public --add-masquerade
# instead of having to specify a rich rule you can add it as a prefix to the zone
root@node0X:$ firewall-cmd --zone=private --add-source=10.0.0.0/8 --permanent
root@node0X:$ firewall-cmd --reload
root@node0X:$ echo "net.ipv4.ip_forward = 1" | tee /etc/sysctl.d/99-ipforward.conf
root@node0X:$ sysctl --system
root@node0X:$ sysctl net.ipv4.ip_forward
```

Allow the nodes to reach eachother's etcd service

```bash
root@node0X:$ firewall-cmd --zone=private --add-port=2379-2380/tcp --permanent && firewall-cmd --reload
```

Allow the nodes to reach eachother's kubeapi

```bash
root@node0X:$ firewall-cmd --zone=private --add-port=6443/tcp --permanent && firewall-cmd --reload
```

Allow the nodes to reach the kubelet service

```bash
root@node0X:$ firewall-cmd --zone=private --add-port=10250/tcp --permanent && firewall-cmd --reload
``` 

On all nodes allow the flannel traffic on udp 8472


```bash
root@node0X:$ firewall-cmd --zone=private --add-port=8472/udp --permanent && firewall-cmd --reload
``` 

allow kube-controller-manager traffic

```bash
root@node0X:$ firewall-cmd --zone=private --add-port=10257/tcp --permanent && firewall-cmd --reload
``` 

allow kube-scheduler traffic

```bash
root@node0X:$ firewall-cmd --zone=private --add-port=10259/tcp --permanent && firewall-cmd --reload
``` 
On all nodes allow the DNS traffic and DNS metrics

```bash
root@node0X:$ firewall-cmd --zone=private --add-port=53/udp --permanent 
root@node0X:$ firewall-cmd --zone=private --add-port=53/tcp --permanent 
root@node0X:$ firewall-cmd --zone=private --add-port=9153/tcp --permanent 
root@node0X:$ firewall-cmd --reload
``` 

### Install the cluster

Now we'll proceed with the actual k8s install. 

```bash
root@node01:$ ansible-playbook -i inventory/mycluster/inventory.ini --become --become-user=root cluster.yml -vv
```

Once that is done you should have the following services installed: 

- Core
  - [kubernetes](https://github.com/kubernetes/kubernetes) 1.33.5 which includes the kube-apiserver, the kube-controller-manager and the kube-scheduler. You can verify this by running:
  ```kubectl get pods -n kube-system -o 'custom-columns=NAME:.metadata.name,VERSION:.spec.containers[*].image' | grep -E 'kube-apiserver|kube-controller-manager|kube-scheduler'```
  - [kubelet](https://github.com/kubernetes/kubelet) 1.33.5 running as a systemd service. You can verify this by running:
  ```/usr/local/bin/kubelet --version```
  - [etcd](https://github.com/etcd-io/etcd) 3.5.22 running as a systemd service. You can verify this by running:
  ``` etcd --version ```
  - [containerd](https://containerd.io/) 2.1.4 running as a systemd service. You can verify this by running:
  ```containerd --version ```
- Network Plugin
  - [flannel](https://github.com/flannel-io/flannel) 0.27.3. You can verify this by running:
  ```kubectl -n kube-system get pods -l app=flannel -o jsonpath='{.items[*].spec.containers[*].image}'```
- Application
  - [coredns](https://github.com/coredns/coredns) 1.12.0. You can verify this by running:
  ```kubectl -n kube-system get deployment coredns -o jsonpath='{.spec.template.spec.containers[*].image}'```


You might run into errors during the ansible script. To revert changes run the following: 

```bash
root@node01:$ ansible-playbook -i inventory/mycluster/inventory.ini --become --become-user=root reset.yml -vv
root@node01:$ dnf install -y netstat
root@node01:$ netstat -tulnp
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      903/sshd: /usr/sbin 
tcp6       0      0 :::22                   :::*                    LISTEN      903/sshd: /usr/sbin 
```

reconfigure the kubeproxy as it doesn't apply my serviceClusterIPRange and doesn't start with iptables for some unknown reason

```bash
root@jump:$ kubectl -n kube-system edit configmap kube-proxy
    apiVersion: kubeproxy.config.k8s.io/v1alpha1
    kind: KubeProxyConfiguration
    clusterCIDR: 10.233.64.0/18
    mode: "iptables"
    serviceClusterIPRange: 10.233.0.0/18

root@jump:$ kubectl -n kube-system delete pod -l k8s-app=kube-proxy
root@jump:$ kubectl rollout restart daemonset nodelocaldns -n kube-system
```


On your computer or jump host: 

```bash
root@jump:$ dnf install zsh fzf net-tools netcat traceroute
root@jump:$ mkdir -p ~/.zsh/completions
root@jump:$ k completion zsh > ~/.zsh/completions/_kubec
root@jump:$ chsh -s /bin/zsh root
root@jump:$ scp hetzner-k8s-node01:/etc/kubernetes/admin.conf ~/.kube/config
root@jump:$ ssh -L 6443:127.0.0.1:6443 hetzner-k8s-node01
```

```bash
root@jump:$ nano .zshrc
# Enable Zsh completion system
autoload -Uz compinit
compinit

# kubectl completion
source <(kubectl completion zsh)
source <(fzf --zsh)

# If you use alias 'k' for kubectl
alias k=kubectl
compdef __start_kubectl k

export TERM=xterm

# Arrow keys (with modifiers)
bindkey '^[[1;5D' backward-word   # Ctrl+Left
bindkey '^[[1;5C' forward-word    # Ctrl+Right
bindkey '^[[1;5A' up-line-or-history
bindkey '^[[1;5B' down-line-or-history

bindkey '^[[H' beginning-of-line
bindkey '^[[F' end-of-line
bindkey '^[[1~' beginning-of-line
bindkey '^[[4~' end-of-line
bindkey '^[[3~' delete-char

# Where to save history
HISTFILE=~/.zsh_history

# How many commands to keep in memory during a session
HISTSIZE=50000

# How many commands to save on disk
SAVEHIST=50000
```

copy over the private key to all cluster members so that they can connect to eachother in case you lose access to one of the nodes

create an alias on your machine to bind the port locally over ssh and fork the process in the background

```bash
root@jump:$ nano .bash_profile
alias kubeconnect='ssh -fNT -L 6443:127.0.0.1:6443 hetzner-k8s-node01'
```

copy the pem key /etc/kubernetes/pki/ca.crt to your computer and install it in your keystore


```bash
root@jump:$ scp hetzner-k8s-node01:/etc/kubernetes/pki/ca.crt /etc/pki/ca-trust/source/anchors
root@jump:$ update-ca-trust extract
root@jump:$ openssl verify -CAfile /etc/pki/tls/certs/ca-bundle.crt /etc/pki/ca-trust/source/anchors/ca.crt
/etc/pki/ca-trust/source/anchors/ca.crt: OK
root@jump:$ nano .kube/config
clusters:
- cluster:
    certificate-authority: /etc/pki/ca-trust/source/anchors/ca.crt
(...)
```

and your cluster should be available on the jumphost

```bash
root@jump:$ ssh -L 6443:127.0.0.1:6443 hetzner-k8s-node01
root@jump:$ k get nodes
NAME         STATUS   ROLES           AGE   VERSION
kube-node1   Ready    control-plane   14m   v1.33.5
kube-node2   Ready    control-plane   13m   v1.33.5
kube-node3   Ready    control-plane   13m   v1.33.5
```

for reasons i can't explain yet, the coredns running on the cluster does not have the service ip i set on kubespray. If that ever happens to you, you can fix it with this:

```bash
root@jump:$ k -n kube-system get svc coredns -o yaml > /tmp/kube-dns.yaml
root@jump:$ nano /tmp/kube-dns.yaml
root@jump:$ k -n kube-system delete svc coredns
root@jump:$ k apply -f /tmp/kube-dns.yaml
root@jump:$ k edit daemonset nodelocaldns -n kube-system
root@jump:$ k rollout restart deployment coredns -n kube-system
root@jump:$ k rollout restart daemonset nodelocaldns -n kube-system
```

for reasons i can't explain yet, the kubelet running on the cluster does not have the right dns To fix this:

```bash
# set the right coredns ip: 10.233.0.10
root@node0X:$ nano /etc/kubernetes/kubelet-config.yaml 
root@node0X:$ systemctl restart kubelet
```



### Conclusion

By combining the setup described here with the knowledge [from our previous article](https://chirpy.thekor.eu/posts/k8s-p2/) you can easily set up a load balancer on hetzner that would manage the TLS certificates for you and balance the load between the 3 machines. 

![iframe](</assets/img/posts/swappy-20250922-164514.png>)
![iframe](</assets/img/posts/swappy-20250922-164613.png>)


Cheers
